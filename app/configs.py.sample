from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC

from data import Data
from sklearn.linear_model import LogisticRegression
import copy

query_strategies = [
    ("max_prob", 1),
]

number_of_papers = None
number_of_iterations = 2
cycle = 20
features_columns_cleaning = ["title", "abstract", "processed_title", "processed_abstract"]

output_dir = 'output'

result = 'result/'

feature_configs = {
    "-": {
        "sampler": None,
        "percentile": 100,
        "prioritize": False,
    },
}

models = {
    "LogisticRegression": LogisticRegression(),
    "NaiveBayes": MultinomialNB(),
    "SVM": SVC(probability=True),
    "RandomForest": RandomForestClassifier(n_estimators=100),
}

new_feature_configs = {}
for key, feature_config in feature_configs.items():
    for key_model, model in models.items():
        new_key = key + '_' + key_model
        new_feature_configs[new_key] = copy.deepcopy(feature_config)
        new_feature_configs[new_key]["model"] = model
feature_configs = copy.deepcopy(new_feature_configs)

feature_extractors = {
    "TFIDF_Low": {
        "tokenizer": "TF-IDF",
        "tokenizer_max_df": 0.7,
        "tokenizer_min_df": 0.2,
    }, "TFIDF_High": {
        "tokenizer": "TF-IDF",
        "tokenizer_max_df": 0.9,
        "tokenizer_min_df": 0.1,
    },
    # "BagOfWords": {
    #     "tokenizer": "BOW",
    # }
}

new_feature_configs = {}
for key, feature_config in feature_configs.items():
    for key_ext, extractor in feature_extractors.items():
        new_key = key + '_' + key_ext
        new_feature_configs[new_key] = copy.deepcopy(feature_config)
        new_feature_configs[new_key] = {**new_feature_configs[new_key], **extractor}
feature_configs = copy.deepcopy(new_feature_configs)

features_before_and_after = {
    "baseline": {
        "feature_before_vectorize": ["title", "abstract"],
        "feature_after_vectorize": ["title", "abstract"],
        "revectorize": False,
    }, "endnote": {
        "feature_before_vectorize": ["processed_title", "processed_abstract", "processed_endnote_pdf_text"],
        "feature_after_vectorize": ["processed_title", "processed_abstract", "processed_endnote_pdf_text"],
        "revectorize": False,
    }, "fulltextpdf": {
        "feature_before_vectorize": ["processed_title", "processed_abstract", "processed_endnote_pdf_text"],
        "feature_after_vectorize": ["processed_title", "processed_abstract", "processed_endnote_pdf_text",
                                    "processed_pdf_manual_pdf_text"],
        "revectorize": True,
    }
}

new_feature_configs = {}
for key, feature_config in feature_configs.items():
    for key_ext, extractor in features_before_and_after.items():
        new_key = key + '_' + key_ext
        new_feature_configs[new_key] = copy.deepcopy(feature_config)
        new_feature_configs[new_key] = {**new_feature_configs[new_key], **extractor}
feature_configs = copy.deepcopy(new_feature_configs)

strategies = {
    "max_probe": None,
    "uncertainty": None,
}

for key, strategy in strategies.items():
    strategies[key] = {
        "configs": copy.deepcopy(feature_configs),
        "step": 1,
    }

label_column_list = ["title_label", "fulltext_label"]
filter_data_list = ["all", "endnote", "fulltext"]
data_set_path_list = {
    "vande": "../asset/pickle_datasets/test_vande_200_papers.pickle",
}

full_configs = {}

for label_column in label_column_list:
    for filter_data in filter_data_list:
        for data_set_name, data_set_path in data_set_path_list.items():
            full_configs[label_column + '_' + filter_data + '_' + data_set_name] = {
                "data": Data(
                    pickle_file=data_set_path,
                    label_column=label_column,
                    features_columns_cleaning=features_columns_cleaning,
                    filter_data=filter_data,
                    cycle=cycle,
                    papers_count=number_of_papers,
                ),
                "strategies": copy.deepcopy(strategies),
            }
